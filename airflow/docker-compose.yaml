version: '3'


services:
  #  airflow-db:
  #    image: postgres:13
  #    container_name: airflow-db
  #    environment:
  #      - POSTGRES_USER=airflow
  #      - POSTGRES_PASSWORD=airflow
  #      - POSTGRES_DB=airflow
  #    healthcheck:
  #      test: [ "CMD", "pg_isready", "-U", "airflow" ]
  #      interval: 5s
  #      retries: 5
  #    volumes:
  #      - postgres_db_volume:/var/lib/postgresql/data
  airflow-init:
    image: apache/airflow:2.10.5
    depends_on:
      - cloud-sql-proxy              # wait until Postgres is reachable
    environment:
      - POSTGRES_PW=${POSTGRES_PW}        
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${POSTGRES_PW}@cloud-sql-proxy:5432/airflow

      #    volumes:
      # (mount your key, dbt folder etc. only if really required here)
    command:
      - bash
      - -c
      - |
        # wait until the proxy answers
        until pg_isready -h cloud-sql-proxy -p 5432 -U airflow; do
          sleep 1
        done
        # migrate once and exit
        airflow db migrate
        airflow users create \
          --username admin --password admin --firstname Admin --lastname User \
          --role Admin --email admin@example.com || true

  cloud-sql-proxy:
    # official v2 container
    image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.10.0
    # run as a daemon listening on all interfaces, port 5432
    #    command: >
    #      /cloud-sql-proxy
    #      ${CLOUD_SQL_CONNECTION_NAME}
    #      --address 0.0.0.0
    #      --port 5432

    # always restart with the rest of the stack
    restart: always
    # host networking so 0.0.0.0:5432 is reachable by sibling containers
    #    network_mode: host
    # use the same service account key file you already mount
    volumes:
      - ../.gcp/gcp_sa_key.json:/config/sa.json:ro
    environment:
      # point proxy at the key
      - GOOGLE_APPLICATION_CREDENTIALS=/config/sa.json
      #      - CLOUD_SQL_CONNECTION_NAME=your-project:us-central1:airflow-postgres
      - CLOUD_SQL_CONNECTION_NAME=data-eng-zcamp-liminm:us-central1:airflow-postgres
    #    command: >
    #      ${CLOUD_SQL_CONNECTION_NAME}=tcp:5432
    #      --credentials-file=/config/sa.json
    command:
      - "--address=0.0.0.0"
      - "--port=5432"
      - "data-eng-zcamp-liminm:us-central1:airflow-postgres"

  airflow-webserver:
#    build:
#      context: .
#      dockerfile: Dockerfile
#    image: airflow-with-dbt:2.10.5
    image: us-central1-docker.pkg.dev/data-eng-zcamp-liminm/airflow-docker/airflow-with-dbt:2.10.5
    container_name: airflow-webserver
    hostname: airflow-webserver
    depends_on:
      - airflow-init
      - cloud-sql-proxy
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__WEBSERVER__WEB_SERVER_MASTER_TIMEOUT=300   # default 120
      - AIRFLOW__WEBSERVER__WORKER_TIMEOUT=300              # default 120
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=xhag_grOHXEReJ-0smqPSepma0R1dDNZJNY_Gvwci1E=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      #      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      #      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${POSTGRES_PW}@localhost:5432/airflow
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${POSTGRES_PW}@cloud-sql-proxy:5432/airflow
      - AIRFLOW_UID=50000
      - AIRFLOW_GID=50000
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/keys/gcp_sa_key.json
      - AIRFLOW__WEBSERVER__SECRET_KEY=your_consistent_secret_key
      #      - DBT_PROFILES_DIR=/home/airflow/.dbt
      - DBT_PROFILES_DIR=/opt/airflow/dbt_project/.dbt


    volumes:
      - ./dags:/opt/airflow/dags
      - ../data_pipeline_dbt:/opt/airflow/dbt_project:ro
      #      - ~/.dbt:/home/airflow/.dbt:ro      # ← mount your real profiles here
      - ../data_pipeline_dbt/.dbt:/opt/airflow/dbt_project/.dbt:ro
      - ./logs:/opt/airflow/logs
      - ../.gcp/gcp_sa_key.json:/opt/airflow/keys/gcp_sa_key.json:ro
      - dbt_target:/opt/airflow/dbt_project/target:rw
    command: >
      bash -c "
      airflow webserver
      "

  #    command: >
  #      bash -c "
  #      airflow db upgrade &&
  #      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
  #      (airflow connections get 'google_cloud_default' || airflow connections add 'google_cloud_default' --conn-type 'google_cloud_platform' --conn-extra '{\"project\": \"data-eng-zcamp-liminm\", \"key_path\": \"/opt/airflow/keys/gcp_sa_key.json\"}') &&
  #      airflow webserver
  #      "

  #    command: >
  #      bash -c "
  #      airflow db migrate &&
  #      airflow api-server
  #      "


  airflow-scheduler:
#    build:
#      context: .
#      dockerfile: Dockerfile
#    image: airflow-with-dbt:2.10.5
    image: us-central1-docker.pkg.dev/data-eng-zcamp-liminm/airflow-docker/airflow-with-dbt:2.10.5
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    depends_on:
      - airflow-init
      - cloud-sql-proxy
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      #      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      #      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${POSTGRES_PW}@localhost:5432/airflow
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${POSTGRES_PW}@cloud-sql-proxy:5432/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_UID=50000
      - AIRFLOW_GID=50000
      - AIRFLOW__WEBSERVER__SECRET_KEY=your_consistent_secret_key
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/keys/gcp_sa_key.json
      - AIRFLOW__CORE__FERNET_KEY=xhag_grOHXEReJ-0smqPSepma0R1dDNZJNY_Gvwci1E=
      #      - DBT_PROFILES_DIR=/home/airflow/.dbt
      - DBT_PROFILES_DIR=/opt/airflow/dbt_project/.dbt


    volumes:
      #      - ./dags:/opt/airflow/dags
      #      - ./logs:/opt/airflow/logs
      #      - ./data_pipeline_dbt:/opt/airflow/dbt_project:ro
      #
      #      # mount your host's ~/.dbt (which contains profiles.yml)
      #      - ~/.dbt:/home/airflow/.dbt:ro
      #
      #      - /home/limin/PycharmProjects/data-engineering-zoomcamp/data-eng-zcamp-liminm-283d5c1e9fbd.json:/opt/airflow/keys/gcp_sa_key.json:ro
      - ./dags:/opt/airflow/dags
      - ../data_pipeline_dbt:/opt/airflow/dbt_project:ro
      - ~/.dbt:/home/airflow/.dbt:ro
      - ./logs:/opt/airflow/logs
      - ../.gcp/gcp_sa_key.json:/opt/airflow/keys/gcp_sa_key.json:ro
      - ./airflow_dbt_logs:/opt/airflow/dbt_project/logs
      - dbt_target:/opt/airflow/dbt_project/target:rw

    command: airflow scheduler
  #
  #    command: >
  #      bash -c "
  #      airflow db upgrade &&
  #      airflow scheduler
  #      "

  airflow-triggerer:
#    build:
#      context: .
#      dockerfile: Dockerfile
#    image: airflow-with-dbt:2.10.5
    image: us-central1-docker.pkg.dev/data-eng-zcamp-liminm/airflow-docker/airflow-with-dbt:2.10.5
    container_name: airflow-triggerer
    hostname: airflow-triggerer
    depends_on:
      - airflow-init
      - cloud-sql-proxy
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      #      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      #      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${POSTGRES_PW}@localhost:5432/airflow
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${POSTGRES_PW}@cloud-sql-proxy:5432/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_UID=50000
      - AIRFLOW_GID=50000
      - AIRFLOW__WEBSERVER__SECRET_KEY=your_consistent_secret_key
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/keys/gcp_sa_key.json
      - AIRFLOW__CORE__FERNET_KEY=xhag_grOHXEReJ-0smqPSepma0R1dDNZJNY_Gvwci1E=

    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ../data_pipeline_dbt:/opt/airflow/dbt_project
      - ../data_pipeline_dbt/.dbt:/opt/airflow/dbt_project/.dbt
      - /home/limin/PycharmProjects/data-engineering-zoomcamp/data-eng-zcamp-liminm-283d5c1e9fbd.json:/opt/airflow/keys/gcp_sa_key.json:ro
      - dbt_target:/opt/airflow/dbt_project/target:rw

    #    command: airflow triggerer
    command: airflow triggerer

#    command: >
#      bash -c "
#      airflow db upgrade &&
#      airflow triggerer
#      "

volumes:
  postgres_db_volume:
  dbt_target:
